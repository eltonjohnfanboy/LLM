{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Custom 8-bit quantizer"
      ],
      "metadata": {
        "id": "Qm6DSMD07ZHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be implementing a per-channel quantization method to quantize the model in 8-bit."
      ],
      "metadata": {
        "id": "aB6K-6Rn7xFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "1CICQjfpKzOk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Init random variables\n",
        "random_int8 = torch.randint(-128, 127, (32, 16)).to(torch.int8)\n",
        "random_hs = torch.randn((1, 16), dtype=torch.bfloat16)\n",
        "scales = torch.randn((1, 32), dtype=torch.bfloat16)\n",
        "bias = torch.randn((1, 32), dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "7l07ujM1K6Cc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def w8_a16_forward(weight, input, scales, bias=None):\n",
        "\n",
        "    casted_weights = weight.to(input.dtype)\n",
        "    output = F.linear(input, casted_weights) * scales\n",
        "\n",
        "    if bias is not None:\n",
        "        output = output + bias\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "_GSDylADMuYY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6HnA8-uW7WpZ"
      },
      "outputs": [],
      "source": [
        "class W8A16LinearLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features, out_features, bias = True, dtype = torch.float32):\n",
        "    super().__init__()\n",
        "    self.register_buffer(\n",
        "            \"int8_weights\",\n",
        "            torch.randint(\n",
        "                -128, 127, (out_features, in_features), dtype=torch.int8\n",
        "            )\n",
        "        )\n",
        "    self.register_buffer(\"scales\",\n",
        "                             torch.randn((out_features), dtype=dtype))\n",
        "\n",
        "    if bias:\n",
        "        self.register_buffer(\"bias\",\n",
        "                              torch.randn((1, out_features),\n",
        "                                          dtype=dtype))\n",
        "    else:\n",
        "        self.bias = None\n",
        "\n",
        "  def quantize(self, weights):\n",
        "        w_fp32 = weights.clone().to(torch.float32)\n",
        "\n",
        "        scales = w_fp32.abs().max(dim=-1).values / 127\n",
        "        scales = scales.to(weights.dtype)\n",
        "\n",
        "        int8_weights = torch.round(weights\n",
        "                        /scales.unsqueeze(1)).to(torch.int8)\n",
        "\n",
        "        self.int8_weights = int8_weights\n",
        "        self.scales = scales\n",
        "\n",
        "  def forward(self, input):\n",
        "      return w8_a16_forward(self.int8_weights,\n",
        "                            input, self.scales, self.bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_linear_with_target(module,\n",
        "                               target_class, module_name_to_exclude):\n",
        "    for name, child in module.named_children():\n",
        "        if isinstance(child, nn.Linear) and not \\\n",
        "          any([x == name for x in module_name_to_exclude]):\n",
        "            old_bias = child.bias\n",
        "\n",
        "            new_module = target_class(child.in_features,\n",
        "                                      child.out_features,\n",
        "                                      old_bias is not None,\n",
        "                                      child.weight.dtype)\n",
        "            setattr(module, name, new_module)\n",
        "            if old_bias is not None:\n",
        "              getattr(module, name).bias = old_bias\n",
        "        else:\n",
        "            # Recursively call the function for nested modules\n",
        "            replace_linear_with_target(\n",
        "                child, target_class, module_name_to_exclude)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecDaqvX9MIbZ",
        "outputId": "142ea6d2-5a0d-4907-9262-1e4725ff19d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W8A16LinearLayer()"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jMRJUGHMIwR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}